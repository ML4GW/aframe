# workflow that builds docker container images
# from the singularity apptainer definition files
# we use for local development. Uses tar archives
# to convert between the formats in order to handle
# the large memory footprints of our containers without
# toppling over the GitHub runner nodes this executes on.
name: container-build 
on: 
  push:
env:
  REGISTRY: ghcr.io

jobs:
  build:
    runs-on: ubuntu-latest
    # only run container build if commit message explicitly asks for it
    if: "contains(github.event.head_commit.message, ':build:')"
    permissions:
      contents: read
      packages: write
    strategy:
      fail-fast: false
      matrix:
        project: [train, export, data]
    steps:
    -
      name: Delete huge unnecessary tools folder
      run: rm -rf /opt/hostedtoolcache 
    -
      name: Checkout repository
      uses: actions/checkout@v4
      with:
        submodules: recursive
    - 
      name: Log in to the Container registry
      uses: docker/login-action@65b78e6e13532edd9afa3aa52ac7964289d1a9c1
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    # build the singularity image as a sandbox directory
    # inside a docker container that has singularity
    # installed (take a big breath). Then tar that directory
    # so that we can import it into docker. Doing everything
    # in one fell swoop because of permissions discrepancies
    # inside and outside the container.
    -
      name: Build singularity image
      run: |
        docker run \
          --rm \
          -v ${{ github.workspace }}:/opt/aframe \
          --workdir /opt/aframe/projects/${{ matrix.project }} \
          --privileged \
          --entrypoint /bin/bash \
          quay.io/singularity/singularity:v3.8.1 \
          -c 'singularity build --sandbox /tmp/app apptainer.def && tar -czf /opt/aframe/app.tar.gz -C /tmp/app .'

    # now copy the fs contents into an empty
    # container and push it to the registry,
    # using a lowercase version of the tag since
    # the github environment variables are case-sensitive
    # TODO: add step to run tests inside docker
    
    # TODO: currently combining path for nvidia variables 
    # necessary to launch triton in export container
    # with path required by train / data projects. 
    # Should add conditionals in this workflow to set these dynamically
    - 
      name: build docker image
      env:
        tag: ${{ env.REGISTRY }}/${{ github.repository }}/${{ matrix.project }}:${{ github.ref_name }}
        path: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/cuda-11.8/bin:/opt/tritonserver/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin
      run: |
        export TAG_LC=${tag,,}
        cat app.tar.gz | docker import --change "ENV PATH=${{ env.path }}" - $TAG_LC
        docker push $TAG_LC
