# Infer
Performing inference with `Aframe` models using NVIDIA's [Triton Inference Server](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html)

## Environment
The infer project is currently not containerized, and only uses `poetry` for environment management.

The infer environment can be built by running 

```
poetry install
```

in the root infer project directory

## Scripts
TODO: explain inference deployment
