Bootstrap: docker
From: nvcr.io/nvidia/tritonserver:22.12-py3
Stage: build

%files
. /opt/aframe/projects/export/
../utils /opt/aframe/projects/utils
../../aframe /opt/aframe/aframe
../../pyproject.toml /opt/aframe/pyproject.toml
../../ml4gw /opt/aframe/ml4gw
../../hermes /opt/aframe/hermes

%post
apt-get update
apt-get install -y --no-install-recommends git

# installing the local package editably via pip
# works, but none of the dependencies (e.g. ml4gw)
# are installed editably. Instead, use poetry export
# to export to a requirements.txt, and use sed 
# to prepend the -e flag to all relevant 
# local packages that require editable installs.
# poetry has already done dependency resolution for us,
# so turn that off with the --no-deps flag to increase performance.
# Lastly, install the export project itself
python3.8 -m pip install poetry
cd /opt/aframe/projects/export
poetry export -o requirements.txt --without-hashes \
    && sed -i 's|\(.*\) @ file://|-e |' requirements.txt
pip install -r requirements.txt --no-deps
pip install -e . --no-deps

# hard code cuda version in container to 11.8.
# tenssorrt 8.5.1.7 is not compatible with cuda > 12.0,
# which is the default on dgx boxes. Sadly, tensorrt 8.6.1 
# is currently incompatible with poetry for some reason
# see this issue https://github.com/NVIDIA/TensorRT/issues/3362.
# once (if) that gets resolved, the more robust fix should be to
# just upgrade tensorrt
%environment
export PATH=/usr/local/cuda-11.8/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH
